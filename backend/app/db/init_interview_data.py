# scripts/init_interview_data.py
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from sqlalchemy.orm import Session
from app.db.database import SessionLocal
from app.models.interview import Interview, InterviewQuestion, InterviewStatistics, InterviewTrendData
from app.models.user import User
from app.models.profile import UserProfile
from datetime import datetime, timedelta
import random
import json

def create_mock_interviews(db: Session, user_id: int, count: int = 10):
    """ä¸ºç”¨æˆ·åˆ›å»ºæ¨¡æ‹Ÿé¢è¯•è®°å½•"""
    positions = ["å‰ç«¯å¼€å‘", "åç«¯å¼€å‘", "äº§å“ç»ç†", "UIè®¾è®¡å¸ˆ", "æ•°æ®åˆ†æ"]
    types = ["practice", "simulation"]
    
    interviews = []
    
    for i in range(count):
        # åˆ›å»ºé¢è¯•è®°å½•
        interview_date = datetime.utcnow() - timedelta(days=random.randint(1, 30))
        
        interview = Interview(
            user_id=user_id,
            type=random.choice(types),
            position=random.choice(positions),
            difficulty=random.choice(["junior", "medium", "senior"]),
            planned_duration=random.choice([30, 45, 60]),
            question_types=["behavioral", "technical"],
            status="completed",
            actual_duration=random.randint(25, 65),
            total_questions=5,
            answered_questions=random.randint(4, 5),
            overall_score=random.uniform(70, 95),
            professional_score=random.uniform(75, 95),
            expression_score=random.uniform(70, 90),
            logic_score=random.uniform(75, 92),
            adaptability_score=random.uniform(65, 88),
            professionalism_score=random.uniform(80, 95),
            started_at=interview_date,
            finished_at=interview_date + timedelta(minutes=random.randint(25, 65))
        )
        
        db.add(interview)
        db.flush()  # è·å–interview.id
        
        # åˆ›å»ºé¢è¯•é¢˜ç›®
        questions_data = [
            {
                "text": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»ï¼ŒåŒ…æ‹¬æ‚¨çš„æ•™è‚²èƒŒæ™¯ã€å·¥ä½œç»éªŒå’ŒæŠ€èƒ½ç‰¹é•¿ã€‚",
                "type": "behavioral",
                "answer": "æˆ‘æ¯•ä¸šäºXXå¤§å­¦è®¡ç®—æœºä¸“ä¸šï¼Œæœ‰3å¹´å‰ç«¯å¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰Vueã€Reactç­‰æ¡†æ¶..."
            },
            {
                "text": f"è¯´è¯´æ‚¨å¯¹{interview.position}çš„ç†è§£ï¼Œä»¥åŠæ‚¨è®¤ä¸ºä¸€ä¸ªä¼˜ç§€çš„å·¥ç¨‹å¸ˆåº”è¯¥å…·å¤‡å“ªäº›èƒ½åŠ›ï¼Ÿ",
                "type": "technical",
                "answer": "æˆ‘è®¤ä¸ºå‰ç«¯å¼€å‘éœ€è¦æŒæ¡æ‰å®çš„åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬HTMLã€CSSã€JavaScript..."
            },
            {
                "text": "è¯·ä»‹ç»ä¸€ä¸ªæ‚¨æœ€æœ‰æˆå°±æ„Ÿçš„é¡¹ç›®ï¼ŒåŒ…æ‹¬é¡¹ç›®èƒŒæ™¯ã€æ‚¨çš„è§’è‰²å’Œè´¡çŒ®ã€‚",
                "type": "project",
                "answer": "æˆ‘å‚ä¸å¼€å‘äº†ä¸€ä¸ªç”µå•†å¹³å°çš„å‰ç«¯é¡¹ç›®ï¼Œè´Ÿè´£å•†å“è¯¦æƒ…é¡µçš„å¼€å‘..."
            },
            {
                "text": "å½“æ‚¨ä¸å›¢é˜Ÿæˆå‘˜æ„è§ä¸ä¸€è‡´æ—¶ï¼Œæ‚¨ä¼šå¦‚ä½•å¤„ç†ï¼Ÿ",
                "type": "situational",
                "answer": "æˆ‘ä¼šé¦–å…ˆå€¾å¬å¯¹æ–¹çš„è§‚ç‚¹ï¼Œç†è§£åˆ†æ­§çš„åŸå› ï¼Œç„¶åé€šè¿‡æ•°æ®å’Œäº‹å®..."
            },
            {
                "text": "æ‚¨æœ‰ä»€ä¹ˆæƒ³é—®æˆ‘çš„å—ï¼Ÿ",
                "type": "behavioral",
                "answer": "æˆ‘æƒ³äº†è§£ä¸€ä¸‹å…¬å¸çš„æŠ€æœ¯æ ˆå’Œå›¢é˜Ÿæ–‡åŒ–..."
            }
        ]
        
        for j, q_data in enumerate(questions_data):
            question = InterviewQuestion(
                interview_id=interview.id,
                question_text=q_data["text"],
                question_type=q_data["type"],
                difficulty=interview.difficulty,
                order_index=j + 1,
                answer_text=q_data["answer"],
                answer_duration=random.randint(60, 180),  # 1-3åˆ†é’Ÿ
                score=random.uniform(3.0, 4.5),
                ai_feedback=json.dumps({
                    "score": random.uniform(3.0, 4.5),
                    "pros": "å›ç­”ç»“æ„æ¸…æ™°ï¼Œè¡¨è¾¾æµç•…ï¼Œé‡ç‚¹çªå‡ºã€‚",
                    "cons": "å¯ä»¥å¢åŠ ä¸€äº›å…·ä½“çš„æ¡ˆä¾‹æ¥æ”¯æ’‘è§‚ç‚¹ã€‚",
                    "reference": "å‚è€ƒç­”æ¡ˆï¼šå»ºè®®ä»å…·ä½“èƒŒæ™¯å¼€å§‹...",
                    "keyword_match": random.uniform(0.7, 0.9),
                    "fluency_score": random.uniform(0.8, 0.95)
                }),
                keyword_match=random.uniform(0.7, 0.9),
                fluency_score=random.uniform(0.8, 0.95),
                asked_at=interview_date + timedelta(minutes=j * 8),
                answered_at=interview_date + timedelta(minutes=j * 8 + 3)
            )
            db.add(question)
        
        interviews.append(interview)
    
    return interviews

def create_user_statistics(db: Session, user_id: int, interviews: list):
    """åˆ›å»ºç”¨æˆ·ç»Ÿè®¡æ•°æ®"""
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    total_interviews = len(interviews)
    total_practice = len([i for i in interviews if i.type == "practice"])
    total_simulation = len([i for i in interviews if i.type == "simulation"])
    total_time = sum([i.actual_duration for i in interviews if i.actual_duration])
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    scores = [i.overall_score for i in interviews if i.overall_score]
    avg_overall = sum(scores) / len(scores) if scores else 0
    
    professional_scores = [i.professional_score for i in interviews if i.professional_score]
    avg_professional = sum(professional_scores) / len(professional_scores) if professional_scores else 0
    
    expression_scores = [i.expression_score for i in interviews if i.expression_score]
    avg_expression = sum(expression_scores) / len(expression_scores) if expression_scores else 0
    
    logic_scores = [i.logic_score for i in interviews if i.logic_score]
    avg_logic = sum(logic_scores) / len(logic_scores) if logic_scores else 0
    
    adaptability_scores = [i.adaptability_score for i in interviews if i.adaptability_score]
    avg_adaptability = sum(adaptability_scores) / len(adaptability_scores) if adaptability_scores else 0
    
    professionalism_scores = [i.professionalism_score for i in interviews if i.professionalism_score]
    avg_professionalism = sum(professionalism_scores) / len(professionalism_scores) if professionalism_scores else 0
    
    # åˆ›å»ºç»Ÿè®¡è®°å½•
    stats = InterviewStatistics(
        user_id=user_id,
        total_interviews=total_interviews,
        total_practice=total_practice,
        total_simulation=total_simulation,
        total_time_minutes=total_time,
        avg_overall_score=avg_overall,
        avg_professional_score=avg_professional,
        avg_expression_score=avg_expression,
        avg_logic_score=avg_logic,
        avg_adaptability_score=avg_adaptability,
        avg_professionalism_score=avg_professionalism,
        score_improvement=random.uniform(8, 15),
        better_than_percent=min(95, 60 + total_interviews * 2),
        last_interview_date=max([i.finished_at for i in interviews if i.finished_at])
    )
    
    db.add(stats)
    return stats

def create_trend_data(db: Session, user_id: int, days: int = 30):
    """åˆ›å»ºè¶‹åŠ¿æ•°æ®"""
    base_score = 75
    
    for i in range(days):
        date = datetime.utcnow() - timedelta(days=days - i)
        
        # æ¨¡æ‹Ÿåˆ†æ•°é€æ¸æå‡çš„è¶‹åŠ¿
        daily_score = base_score + random.uniform(-3, 5)
        base_score = min(95, base_score + 0.3)  # æ¯å¤©å¾®å°æå‡
        
        trend = InterviewTrendData(
            user_id=user_id,
            record_date=date,
            daily_score=daily_score,
            interviews_count=random.choice([0, 0, 0, 1, 1, 2]),  # å¤§éƒ¨åˆ†å¤©æ•°æ²¡æœ‰é¢è¯•
            cumulative_avg_score=daily_score,
            professional_score=daily_score + random.uniform(-3, 3),
            expression_score=daily_score + random.uniform(-2, 2),
            logic_score=daily_score + random.uniform(-4, 4),
            adaptability_score=daily_score + random.uniform(-5, 5),
            professionalism_score=daily_score + random.uniform(-2, 2)
        )
        
        db.add(trend)

def init_mock_data_for_user(db: Session, username: str):
    """ä¸ºæŒ‡å®šç”¨æˆ·åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®"""
    # æŸ¥æ‰¾ç”¨æˆ·
    user = db.query(User).filter(User.username == username).first()
    if not user:
        print(f"âŒ ç”¨æˆ· {username} ä¸å­˜åœ¨")
        return False
    
    print(f"ğŸ“Š å¼€å§‹ä¸ºç”¨æˆ· {username} (ID: {user.id}) åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    existing_interviews = db.query(Interview).filter(Interview.user_id == user.id).count()
    if existing_interviews > 0:
        print(f"âš ï¸  ç”¨æˆ·å·²æœ‰ {existing_interviews} æ¡é¢è¯•è®°å½•ï¼Œæ˜¯å¦æ¸…é™¤å¹¶é‡æ–°åˆ›å»ºï¼Ÿ(y/n)")
        choice = input().lower()
        if choice == 'y':
            # åˆ é™¤ç°æœ‰æ•°æ®
            db.query(InterviewTrendData).filter(InterviewTrendData.user_id == user.id).delete()
            db.query(InterviewStatistics).filter(InterviewStatistics.user_id == user.id).delete()
            db.query(InterviewQuestion).filter(
                InterviewQuestion.interview_id.in_(
                    db.query(Interview.id).filter(Interview.user_id == user.id)
                )
            ).delete(synchronize_session=False)
            db.query(Interview).filter(Interview.user_id == user.id).delete()
            print("ğŸ—‘ï¸  å·²æ¸…é™¤ç°æœ‰æ•°æ®")
        else:
            print("âŒ å–æ¶ˆæ“ä½œ")
            return False
    
    try:
        # 1. åˆ›å»ºé¢è¯•è®°å½•
        print("1ï¸âƒ£  åˆ›å»ºé¢è¯•è®°å½•...")
        interviews = create_mock_interviews(db, user.id, 15)  # åˆ›å»º15æ¬¡é¢è¯•
        
        # 2. åˆ›å»ºç»Ÿè®¡æ•°æ®
        print("2ï¸âƒ£  ç”Ÿæˆç»Ÿè®¡æ•°æ®...")
        stats = create_user_statistics(db, user.id, interviews)
        
        # 3. åˆ›å»ºè¶‹åŠ¿æ•°æ®
        print("3ï¸âƒ£  ç”Ÿæˆè¶‹åŠ¿æ•°æ®...")
        create_trend_data(db, user.id, 30)  # 30å¤©çš„è¶‹åŠ¿
        
        db.commit()
        
        print("âœ… æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºå®Œæˆï¼")
        print(f"ğŸ“ˆ æ€»è®¡åˆ›å»ºï¼š")
        print(f"   - é¢è¯•è®°å½•ï¼š{len(interviews)} æ¡")
        print(f"   - é¢è¯•é¢˜ç›®ï¼š{len(interviews) * 5} é“")
        print(f"   - è¶‹åŠ¿æ•°æ®ï¼š30 å¤©")
        print(f"   - å¹³å‡è¯„åˆ†ï¼š{stats.avg_overall_score:.1f}")
        
        return True
        
    except Exception as e:
        db.rollback()
        print(f"âŒ åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    db = SessionLocal()
    try:
        print("ğŸ¯ é¢è¯•æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨")
        print("=" * 50)
        
        # åˆ—å‡ºæ‰€æœ‰ç”¨æˆ·
        users = db.query(User).all()
        if not users:
            print("âŒ æ•°æ®åº“ä¸­æ²¡æœ‰ç”¨æˆ·ï¼Œè¯·å…ˆåˆ›å»ºç”¨æˆ·è´¦å·")
            return
        
        print("ğŸ“‹ ç°æœ‰ç”¨æˆ·åˆ—è¡¨ï¼š")
        for i, user in enumerate(users, 1):
            interview_count = db.query(Interview).filter(Interview.user_id == user.id).count()
            print(f"   {i}. {user.username} (ID: {user.id}) - å·²æœ‰{interview_count}æ¡é¢è¯•è®°å½•")
        
        print("\nè¯·é€‰æ‹©æ“ä½œï¼š")
        print("1. ä¸ºç‰¹å®šç”¨æˆ·åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®")
        print("2. ä¸ºæ‰€æœ‰ç”¨æˆ·åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®")
        print("3. é€€å‡º")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1-3): ").strip()
        
        if choice == "1":
            username = input("è¯·è¾“å…¥ç”¨æˆ·å: ").strip()
            init_mock_data_for_user(db, username)
            
        elif choice == "2":
            for user in users:
                print(f"\nğŸ”„ å¤„ç†ç”¨æˆ·: {user.username}")
                init_mock_data_for_user(db, user.username)
                
        elif choice == "3":
            print("ğŸ‘‹ é€€å‡ºç¨‹åº")
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    main()